Thompson sampling: some intuition and code
========================================================
author: Christopher Beckham
date: 

k-armed bandit problem
========================================================

- We have $k$ levers we can pull at each time step $1, \dots, T$, denoted as actions $a \in A$
- Each action $a_{i} \in A$ gives us a reward drawn from some unknown distribution $r_{i} \sim p(r \ | \ A = a_{i})$
- Goal is to maximise the rewards $Q_{T} = \sum_{i=1}^{T} r_{i}$

Exploration vs exploitation
========================================================

- We are faced with the exploration vs exploitation dilemma
- On one hand, we want to make sure we get the 'best' lever (need to 'explore', in order to gain confidence)
- On the other hand, we want to make sure we get the best reward (need to 'exploit')

A simple bandit algorithm
========================================================

![image](simple_bandit_algorithm.png)

- $\epsilon$ controls the exploration vs exploitation tradeoff

A simple example
========================================================

- Let us consider a 5-armed bandit problem, where $p(r \ | \ A = a_{i}) \sim N(\mu_{i}, 1)$
- $\mu_{i} \sim N(0,1)$

Code
========================================================

```{r, fig.show="hide"}
library(vioplot)
set.seed(2)
k = 5 # 5-armed bandit problem
means = rnorm(k, 0, 1)
n = 1000
vioplot(
  rnorm(n, means[1], 1),
  rnorm(n, means[2], 1),
  rnorm(n, means[3], 1),
  rnorm(n, means[4], 1),
  rnorm(n, means[5], 1),
  names=1:5,
  col="grey"
)
abline(h=0, lty="dashed")
```

Code
========================================================

```{r, echo=FALSE}
vioplot(
  rnorm(n, means[1], 1),
  rnorm(n, means[2], 1),
  rnorm(n, means[3], 1),
  rnorm(n, means[4], 1),
  rnorm(n, means[5], 1),
  names=1:5,
  col="grey"
)
abline(h=0, lty="dashed")
```

eps-greedy code
========================================================

<font size="6">

```{r}
library(nnet)
eps.greedy = function(means, eps, num.iters) {
  k = length(means)
  Q = rep(0, k)
  N = rep(0, k)
  avg.rewards = rep(0, num.iters)
  for(iter in 1:num.iters) {
    if(runif(1,0,1) < eps) {
      A = sample(1:k, 1)
    } else {
      A = which.is.max(Q)
    }
    R = rnorm(1, means[A], 1)
    N[A] = N[A] + 1
    Q[A] = Q[A] + ((1/N[A])*(R - Q[A]))
    # compute average reward
    avg.rewards[iter] = ((N / sum(N)) %*% Q)[1]
  }
  return(avg.rewards)
}
```

</font>

Experimenting with epsilon
========================================================

```{r, echo=FALSE, fig.height=8, fig.width=14}
set.seed(1)
num.reps = 30
plot( rowSums(replicate(num.reps, {eps.greedy(means, 0.1, 1000)})) / num.reps, xlab="time", ylab="average reward", type="l", ylim=c(-1,2))
lines(  rowSums(replicate(num.reps, {eps.greedy(means, 0.01, 1000)})) / num.reps ,type="l",col="red")
lines(  rowSums(replicate(num.reps, {eps.greedy(means, 0, 1000)})) / num.reps ,type="l", col="green")
lines(  rowSums(replicate(num.reps, {eps.greedy(means, 1, 1000)})) / num.reps ,type="l", lty="dashed", col="black")
legend("bottomright",legend=c("eps=0.1", "eps=0.01", "eps=0", "eps=1"),col=c("black","red","green","black"),lty=c("solid","solid","solid","dashed"),lwd=2)
```

Thompson sampling
========================================================

- Let $\mu_{i}$ denote the mean reward of arm $i$
- We can imagine that each arm $i$ draws a reward from a probability distribution $p(r \ | \ \mu_{i} = \theta)$
- It would be nice to know $p(\mu_{i} = \theta \ | \ R)$; that is, what is the probability the mean reward of arm $i$ is $\theta$ given we have observed rewards $R$
- From Bayes' rule: $p(\mu_{i} = \theta \ | \ R) \propto p(R \ | \ \mu_{i} = \theta) \times p(\mu_{i} = \theta)$

Thompson sampling
========================================================

- If we use conjugate priors, then we can compute $p(\mu = \theta \ | \ R)$ in closed form, i.e., update our beliefs based on observed data
- E.g. if our likelihood is $Bernoulli$ and prior is $Beta$, then the posterior is also $Beta$
- Or, if our likelihood and prior are both $Normal$, then the posterior is $Normal$

Bernoulli example
========================================================

- Suppose we observe simply one reward $r \sim Bernoulli(r; \theta)$ and our prior is some $Beta(\theta; \alpha, \beta)$
- The posterior $p(\mu = \theta \ | \ r) = Beta(\theta; \alpha + r, \beta + 1 - r)$
- Now suppose we observe multiple rewards $R_{n} = \{r_{i}\}_{i=1}^{n}$
- The posterior then becomes $Beta(\theta; \alpha + S_{n}, \beta + F_{n})$, where $S_{n} = \sum_{i=1}^{n} 1_{r_{i} = 1}$ and $F_{n} = \sum_{i=1}^{n} 1_{r_{i} \neq 1}$
- (Note: we have a posterior for every arm)

Gaussian example
========================================================

- Suppose we observe rewards $R_{n} = \{r_{i}\}_{i=1}^{n}$, where $r_{i} \sim N(\theta, 1)$ (the variance is known, $\theta$ unknown!)
- Suppose our prior is also Gaussian, $p(\theta) = N(0, 1)$
- Then, the posterior after observing $n$ rewards is $p(\theta \ | \ R_{n}) = N(\mu_{n}, \frac{1}{n+1})$, where $\mu_{n}$ is the mean number of rewards from the beginning to time $n$

Pseudo-code
========================================================

- https://bandits.wikischolars.columbia.edu/file/view/Lecture+4.pdf

![image3](thompson_gaussian.png)

Real code
========================================================

<font size="5">

```{r}
thompson.gauss = function(means, num.iters) {
  # prior is an N(0,1)
  mean.prior = 0
  sd.prior = 1
  Q = rep(0, k)
  N = rep(0, k)
  avg.rewards = rep(0, num.iters)
  for(iter in 1:num.iters) {
    # for each arm i = 1...k, sample \theta_i from posterior distribution
    theta.samples = c()
    for(a in 1:k) {
      theta.samples = c(theta.samples, rnorm(1, Q[a], 1 / (N[a]+1) ))
    }
    # choose the arm that is the argmax of these samples
    A = which.max(theta.samples)
    R = rnorm(1, means[A], 1)
    N[A] = N[A] + 1
    Q[A] = Q[A] + ((1/N[A])*(R - Q[A]))
    # compute average reward
    avg.rewards[iter] = ((N / sum(N)) %*% Q)[1]
  }
  return(avg.rewards)
}
```

</font>

Experiments
========================================================

```{r, echo=FALSE, fig.height=8, fig.width=14}
# look at thompson sampling
set.seed(0)
num.iters = 10
plot( rowSums(replicate(num.iters, {thompson.gauss(means, 1000)})) / num.iters, type="l", ylim=c(-1,1.5), xlab="time", ylab="average reward")
lines(rowSums(replicate(num.iters, {eps.greedy(means, 0.1, 1000)})) / num.iters, type="l",col="blue")
lines(rowSums(replicate(num.iters, {eps.greedy(means, 0.01, 1000)})) / num.iters, type="l",col="red")
lines(rowSums(replicate(num.iters, {eps.greedy(means, 0, 1000)})) / num.iters, type="l",col="green")
lines(rowSums(replicate(num.iters, {eps.greedy(means, 1, 1000)})) / num.iters, type="l", lty="dashed", col="black")
legend("bottomright",legend=c("thompson", "eps=0.1", "eps=0.01", "eps=0", "eps=1"),col=c("black","blue","red","green","black"),lty=c("solid","solid","solid","solid","dashed"),lwd=2)
```

Limitations
========================================================

- Assumption is that the rewards are sampled from some known distribution (e.g. Bernoulli/Normal/Poisson)
- Can still use this algorithm, even if we make false assumptions about the reward distribution

A good reference
========================================================

- https://en.wikipedia.org/wiki/Conjugate_prior

![image2](wikipedia.png)
